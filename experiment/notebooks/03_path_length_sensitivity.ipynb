{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decda771",
   "metadata": {},
   "source": [
    "# 03_path_length_sensitivity.ipynb\n",
    "\n",
    "**Claim under test:**  \n",
    "Pathwise regret scales ~ linearly with P_T at fixed \u03bb and G.\n",
    "\n",
    "**Grid file:** `grids/03_path_length_sensitivity.yaml`\n",
    "\n",
    "**Baseline & conventions**\n",
    "- Use the STAR schema built by `experiment/utils/duck-db-loader.py`.\n",
    "- Slice results by `(grid_id, seed)` using `analytics.v_run_summary`.\n",
    "- Keep plots identical across notebooks; only the *dial* under test changes.\n",
    "- Reference path-length/regret-decomposition companion analysis for context. :contentReference[oaicite:0]{index=0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce4688",
   "metadata": {},
   "source": [
    "## 0) Imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e275d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import duckdb\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# go up one level\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# project paths (edit if your repo layout differs)\n",
    "REPO = Path(\".\").resolve()\n",
    "RESULTS_GLOB = \"results_parquet/03_path_length_sensitivity/events/grid_id=*/seed=*/*.parquet\"  # corrected pattern\n",
    "DB_PATH = REPO/\"artifacts\"/\"star.duckdb\"\n",
    "DB_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GRID_FILE = REPO/\"grids/03_path_length_sensitivity.yaml\"     # this notebook\u2019s grid\n",
    "STAGING_TABLE = \"staging.events\"   # keep default from loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea29652",
   "metadata": {},
   "source": [
    "## 1) Build/load the STAR schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24735927",
   "metadata": {},
   "outputs": [
    {
     "ename": "IOException",
     "evalue": "IO Error: Could not set lock on file \"/workspaces/unlearning-research-meta/experiment/artifacts/star.duckdb\": Conflicting lock is held in /usr/local/bin/python3.11 (PID 21902). See also https://duckdb.org/docs/stable/connect/concurrency",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIOException\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mduck_db_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_star_schema\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m con = \u001b[43mload_star_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRESULTS_GLOB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDB_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstaging_table\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSTAGING_TABLE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_ddl\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_events_view\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# sanity: counts\u001b[39;00m\n\u001b[32m     12\u001b[39m display(con.execute(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33mSELECT \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdim_run\u001b[39m\u001b[33m'\u001b[39m\u001b[33m t, COUNT(*) c FROM analytics.dim_run\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33mUNION ALL SELECT \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfact_event\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, COUNT(*) FROM analytics.fact_event\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33mUNION ALL SELECT \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdim_event_type\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, COUNT(*) FROM analytics.dim_event_type\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m\"\"\"\u001b[39m).df())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/unlearning-research-meta/experiment/utils/duck_db_loader.py:51\u001b[39m, in \u001b[36mload_star_schema\u001b[39m\u001b[34m(input_path, db_path, staging_table, run_ddl, create_events_view, include_parameters, parameters_path)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Create connection\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m db_path:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     conn = \u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     conn = duckdb.connect()\n",
      "\u001b[31mIOException\u001b[39m: IO Error: Could not set lock on file \"/workspaces/unlearning-research-meta/experiment/artifacts/star.duckdb\": Conflicting lock is held in /usr/local/bin/python3.11 (PID 21902). See also https://duckdb.org/docs/stable/connect/concurrency"
     ]
    }
   ],
   "source": [
    "from utils.duck_db_loader import load_star_schema\n",
    "\n",
    "con = load_star_schema(\n",
    "    input_path=str(RESULTS_GLOB),\n",
    "    db_path=str(DB_PATH),\n",
    "    staging_table=STAGING_TABLE,\n",
    "    run_ddl=True,\n",
    "    create_events_view=True,\n",
    ")\n",
    "\n",
    "# sanity: counts\n",
    "display(con.execute(\"\"\"\n",
    "SELECT 'dim_run' t, COUNT(*) c FROM analytics.dim_run\n",
    "UNION ALL SELECT 'fact_event', COUNT(*) FROM analytics.fact_event\n",
    "UNION ALL SELECT 'dim_event_type', COUNT(*) FROM analytics.dim_event_type\n",
    "\"\"\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c12f41",
   "metadata": {},
   "source": [
    "## 2) Snapshot config & runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4415fd0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# show the grid dictionary if kept in YAML (optional)\n",
    "try:\n",
    "    import yaml, textwrap\n",
    "    cfg = yaml.safe_load(Path(GRID_FILE).read_text())\n",
    "    print(\"grid file:\", GRID_FILE)\n",
    "    print(json.dumps(cfg.get(\"matrix\", {}), indent=2))\n",
    "except Exception as e:\n",
    "    print(\"Note: could not parse YAML grid:\", e)\n",
    "\n",
    "runs = con.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM analytics.v_run_summary\n",
    "ORDER BY grid_id, seed\n",
    "\"\"\").df()\n",
    "runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497a6fc",
   "metadata": {},
   "source": [
    "## 3) Standard query helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74757f9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def last_event_frame(con, grid_id:str, seed:int):\n",
    "    return con.execute(\"\"\"\n",
    "    WITH last AS (\n",
    "      SELECT MAX(event_id) AS last_id\n",
    "      FROM analytics.fact_event fe\n",
    "      WHERE fe.grid_id = ? AND fe.seed = ?\n",
    "    )\n",
    "    SELECT fe.*\n",
    "    FROM analytics.fact_event fe\n",
    "    CROSS JOIN last l\n",
    "    WHERE fe.event_id = l.last_id AND fe.grid_id = ? AND fe.seed = ?\n",
    "    \"\"\", [grid_id, seed, grid_id, seed]).df()\n",
    "\n",
    "def trace_frame(con, grid_id:str, seed:int, cols:tuple[str,...]):\n",
    "    col_list = \", \".join(cols)\n",
    "    return con.execute(f\"\"\"\n",
    "    SELECT event_id, {col_list}\n",
    "    FROM analytics.fact_event fe\n",
    "    WHERE fe.grid_id = ? AND fe.seed = ?\n",
    "    ORDER BY event_id\n",
    "    \"\"\", [grid_id, seed]).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbeaff2",
   "metadata": {},
   "source": [
    "## 4) Plots (standardized)\n",
    "Keep styling consistent across notebooks for visual comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08b884",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_traces(df, ycols, title):\n",
    "    for y in ycols:\n",
    "        plt.figure()\n",
    "        plt.plot(df[\"event_id\"], df[y], label=y)\n",
    "        plt.xlabel(\"event_id\"); plt.ylabel(y); plt.title(f\"{title}: {y}\")\n",
    "        plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad919e3",
   "metadata": {},
   "source": [
    "## 5) Notebook-specific check: claim, dial, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e621174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_CLAIM = \"Pathwise regret scales ~ linearly with P_T at fixed \u03bb and G.\"\n",
    "print(\"Claim under test:\", NOTEBOOK_CLAIM)\n",
    "\n",
    "# choose a (grid_id, seed) to visualize (edit if multiple)\n",
    "if not runs.empty:\n",
    "    gid, seed = runs.loc[0, [\"grid_id\",\"seed\"]]\n",
    "    print(\"Example run:\", gid, seed)\n",
    "\n",
    "    # universal traces\n",
    "    df = trace_frame(con, gid, int(seed), (\n",
    "        \"cum_regret\", \"regret_static_term\", \"regret_path_term\",\n",
    "        \"P_T_true\", \"ST_running\", \"g_norm\", \"eta_t\"\n",
    "    ))\n",
    "    plot_traces(df, [\"cum_regret\",\"regret_static_term\",\"regret_path_term\"], \"Regret decomposition\")\n",
    "    plot_traces(df, [\"P_T_true\",\"ST_running\"], \"Path & energy\")\n",
    "    plot_traces(df, [\"g_norm\",\"eta_t\"], \"Grad norm & step size\")\n",
    "\n",
    "    # end-of-run snapshot\n",
    "    tail = last_event_frame(con, gid, int(seed))\n",
    "    display(tail[[\"cum_regret\",\"cum_regret_with_noise\",\"P_T_true\",\"ST_running\",\"rho_spent\",\"m_used\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b959e",
   "metadata": {},
   "source": [
    "## 5.5) Standardized Analysis Follow-ups\n",
    "Execute common analyses across all experiment notebooks for automated claim checks and theory validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Import standardized analysis functions\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from standardized_analysis import run_all_standardized_analyses, enhance_claim_check_export\n",
    "\n",
    "# Run all standardized analyses\n",
    "analysis_results = run_all_standardized_analyses(con, runs)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n=== THEORY BOUND TRACKING ===\")\n",
    "theory_results = analysis_results['theory_bound_tracking']\n",
    "successful_theory = [r for r in theory_results.values() if r['status'] == 'success']\n",
    "if successful_theory:\n",
    "    ratios = [r['theory_ratio_final'] for r in successful_theory if r['theory_ratio_final'] is not None]\n",
    "    if ratios:\n",
    "        print(f\"Theory ratio - Mean: {np.mean(ratios):.3f}, Median: {np.median(ratios):.3f}\")\n",
    "        print(f\"Expected: O(1) when theory matches experiment\")\n",
    "        \n",
    "print(\"\\n=== STEPSIZE POLICY VALIDATION ===\")\n",
    "stepsize_results = analysis_results['stepsize_policy_validation']\n",
    "policy_pass = sum(1 for r in stepsize_results.values() if r['stepsize_policy_status'] == 'pass')\n",
    "policy_total = len([r for r in stepsize_results.values() if r['stepsize_policy_status'] in ['pass', 'fail']])\n",
    "if policy_total > 0:\n",
    "    print(f\"Stepsize policy adherence: {policy_pass}/{policy_total} runs passed\")\n",
    "    \n",
    "print(\"\\n=== PRIVACY & ODOMETER SANITY CHECKS ===\")\n",
    "privacy_results = analysis_results['privacy_odometer_checks']\n",
    "privacy_pass = sum(1 for r in privacy_results.values() if r['privacy_odometer_status'] == 'pass')\n",
    "privacy_total = len([r for r in privacy_results.values() if r['privacy_odometer_status'] in ['pass', 'fail']])\n",
    "if privacy_total > 0:\n",
    "    print(f\"Privacy/Odometer checks: {privacy_pass}/{privacy_total} runs passed\")\n",
    "    \n",
    "print(\"\\n=== SEED STABILITY AUDIT ===\")\n",
    "stability_results = analysis_results['seed_stability_audit']\n",
    "flagged_grids = [grid for grid, stats in stability_results.items() \n",
    "                if stats.get('high_variability_flag', False)]\n",
    "total_grids = len([s for s in stability_results.values() if s['status'] == 'success'])\n",
    "print(f\"High variability grids: {len(flagged_grids)}/{total_grids}\")\n",
    "if flagged_grids:\n",
    "    print(f\"Flagged grids: {flagged_grids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfa17b",
   "metadata": {},
   "source": [
    "## 6) One-page \u201cclaim check\u201d (export)\n",
    "Emits a compact JSON summary to artifacts/ for CI diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747aae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT = REPO/\"artifacts\"/(Path(\"03_path_length_sensitivity.ipynb\").stem + \"_claim_check.json\")\n",
    "ARTIFACT.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get base summary\n",
    "summary = con.execute(\"\"\"\n",
    "SELECT\n",
    "  dr.grid_id, dr.seed,\n",
    "  MAX(fe.cum_regret) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_cum_regret,\n",
    "  MAX(fe.cum_regret_with_noise) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_cum_regret_with_noise,\n",
    "  MAX(fe.P_T_true) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_P_T_true,\n",
    "  MAX(fe.ST_running) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_ST_running,\n",
    "  MAX(fe.rho_spent) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_rho_spent,\n",
    "  MAX(fe.m_used) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_m_used\n",
    "FROM analytics.fact_event fe\n",
    "JOIN analytics.dim_run dr USING (grid_id, seed)\n",
    "GROUP BY dr.grid_id, dr.seed\n",
    "ORDER BY dr.grid_id, dr.seed\n",
    "\"\"\").df().to_dict(orient=\"records\")\n",
    "\n",
    "# Enhance summary with standardized analysis results\n",
    "enhanced_summary = enhance_claim_check_export(\n",
    "    summary, \n",
    "    analysis_results[\"theory_bound_tracking\"],\n",
    "    analysis_results[\"stepsize_policy_validation\"],\n",
    "    analysis_results[\"privacy_odometer_checks\"],\n",
    "    analysis_results[\"seed_stability_audit\"]\n",
    ")\n",
    "\n",
    "ARTIFACT.write_text(json.dumps({\n",
    "    \"notebook\": \"03_path_length_sensitivity.ipynb\",\n",
    "    \"claim\": NOTEBOOK_CLAIM,\n",
    "    \"grid_file\": str(GRID_FILE),\n",
    "    \"summary\": enhanced_summary,\n",
    "    \"standardized_analyses\": {\n",
    "        \"theory_bound_tracking\": analysis_results[\"theory_bound_tracking\"],\n",
    "        \"stepsize_policy_validation\": analysis_results[\"stepsize_policy_validation\"],\n",
    "        \"privacy_odometer_checks\": analysis_results[\"privacy_odometer_checks\"],\n",
    "        \"seed_stability_audit\": analysis_results[\"seed_stability_audit\"]\n",
    "    }\n",
    "}, indent=2))\n",
    "print(\"Wrote:\", ARTIFACT)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}