{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff18dad8",
   "metadata": {},
   "source": [
    "# 04_stepsize_policy_ablation.ipynb\n",
    "\n",
    "**Claim under test:**  \n",
    "Changing stepsize policy alters stability and regret without changing geometry.\n",
    "\n",
    "**Grid file:** `grids/04_stepsize_ablation.yaml`\n",
    "\n",
    "**Baseline & conventions**\n",
    "- Use the STAR schema built by `experiment/utils/duck-db-loader.py`.\n",
    "- Slice results by `(grid_id, seed)` using `analytics.v_run_summary`.\n",
    "- Keep plots identical across notebooks; only the *dial* under test changes.\n",
    "- Reference path-length/regret-decomposition companion analysis for context. :contentReference[oaicite:0]{index=0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2556a",
   "metadata": {},
   "source": [
    "## 0) Imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed51c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import duckdb\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# go up one level\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# project paths (edit if your repo layout differs)\n",
    "REPO = Path(\".\").resolve()\n",
    "RESULTS_GLOB = \"results_parquet/04_stepsize_ablation/events/grid_id=*/seed=*/*.parquet\"  # corrected pattern\n",
    "DB_PATH = REPO/\"artifacts\"/\"star.duckdb\"\n",
    "DB_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GRID_FILE = REPO/\"grids/04_stepsize_ablation.yaml\"     # this notebook\u2019s grid\n",
    "STAGING_TABLE = \"staging.events\"   # keep default from loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b64b8",
   "metadata": {},
   "source": [
    "## 1) Build/load the STAR schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39982901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.duck_db_loader import load_star_schema\n",
    "\n",
    "con = load_star_schema(\n",
    "    input_path=str(RESULTS_GLOB),\n",
    "    db_path=str(DB_PATH),\n",
    "    staging_table=STAGING_TABLE,\n",
    "    run_ddl=True,\n",
    "    create_events_view=True,\n",
    ")\n",
    "\n",
    "# sanity: counts\n",
    "display(con.execute(\"\"\"\n",
    "SELECT 'dim_run' t, COUNT(*) c FROM analytics.dim_run\n",
    "UNION ALL SELECT 'fact_event', COUNT(*) FROM analytics.fact_event\n",
    "UNION ALL SELECT 'dim_event_type', COUNT(*) FROM analytics.dim_event_type\n",
    "\"\"\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0078c",
   "metadata": {},
   "source": [
    "## 2) Snapshot config & runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38521ea6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# show the grid dictionary if kept in YAML (optional)\n",
    "try:\n",
    "    import yaml, textwrap\n",
    "    cfg = yaml.safe_load(Path(GRID_FILE).read_text())\n",
    "    print(\"grid file:\", GRID_FILE)\n",
    "    print(json.dumps(cfg.get(\"matrix\", {}), indent=2))\n",
    "except Exception as e:\n",
    "    print(\"Note: could not parse YAML grid:\", e)\n",
    "\n",
    "runs = con.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM analytics.v_run_summary\n",
    "ORDER BY grid_id, seed\n",
    "\"\"\").df()\n",
    "runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ddeaa",
   "metadata": {},
   "source": [
    "## 3) Standard query helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324b27e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def last_event_frame(con, grid_id:str, seed:int):\n",
    "    return con.execute(\"\"\"\n",
    "    WITH last AS (\n",
    "      SELECT MAX(event_id) AS last_id\n",
    "      FROM analytics.fact_event fe\n",
    "      WHERE fe.grid_id = ? AND fe.seed = ?\n",
    "    )\n",
    "    SELECT fe.*\n",
    "    FROM analytics.fact_event fe\n",
    "    CROSS JOIN last l\n",
    "    WHERE fe.event_id = l.last_id AND fe.grid_id = ? AND fe.seed = ?\n",
    "    \"\"\", [grid_id, seed, grid_id, seed]).df()\n",
    "\n",
    "def trace_frame(con, grid_id:str, seed:int, cols:tuple[str,...]):\n",
    "    col_list = \", \".join(cols)\n",
    "    return con.execute(f\"\"\"\n",
    "    SELECT event_id, {col_list}\n",
    "    FROM analytics.fact_event fe\n",
    "    WHERE fe.grid_id = ? AND fe.seed = ?\n",
    "    ORDER BY event_id\n",
    "    \"\"\", [grid_id, seed]).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd71f93",
   "metadata": {},
   "source": [
    "## 4) Plots (standardized)\n",
    "Keep styling consistent across notebooks for visual comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50112287",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_traces(df, ycols, title):\n",
    "    for y in ycols:\n",
    "        plt.figure()\n",
    "        plt.plot(df[\"event_id\"], df[y], label=y)\n",
    "        plt.xlabel(\"event_id\"); plt.ylabel(y); plt.title(f\"{title}: {y}\")\n",
    "        plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b755a",
   "metadata": {},
   "source": [
    "## 5) Notebook-specific check: claim, dial, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_CLAIM = \"Changing stepsize policy alters stability and regret without changing geometry.\"\n",
    "print(\"Claim under test:\", NOTEBOOK_CLAIM)\n",
    "\n",
    "# choose a (grid_id, seed) to visualize (edit if multiple)\n",
    "if not runs.empty:\n",
    "    gid, seed = runs.loc[0, [\"grid_id\",\"seed\"]]\n",
    "    print(\"Example run:\", gid, seed)\n",
    "\n",
    "    # universal traces\n",
    "    df = trace_frame(con, gid, int(seed), (\n",
    "        \"cum_regret\", \"regret_static_term\", \"regret_path_term\",\n",
    "        \"P_T_true\", \"ST_running\", \"g_norm\", \"eta_t\"\n",
    "    ))\n",
    "    plot_traces(df, [\"cum_regret\",\"regret_static_term\",\"regret_path_term\"], \"Regret decomposition\")\n",
    "    plot_traces(df, [\"P_T_true\",\"ST_running\"], \"Path & energy\")\n",
    "    plot_traces(df, [\"g_norm\",\"eta_t\"], \"Grad norm & step size\")\n",
    "\n",
    "    # end-of-run snapshot\n",
    "    tail = last_event_frame(con, gid, int(seed))\n",
    "    display(tail[[\"cum_regret\",\"cum_regret_with_noise\",\"P_T_true\",\"ST_running\",\"rho_spent\",\"m_used\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b959e",
   "metadata": {},
   "source": [
    "## 5.5) Standardized Analysis Follow-ups\n",
    "Execute common analyses across all experiment notebooks for automated claim checks and theory validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Import standardized analysis functions\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from standardized_analysis import run_all_standardized_analyses, enhance_claim_check_export\n",
    "\n",
    "# Run all standardized analyses\n",
    "analysis_results = run_all_standardized_analyses(con, runs)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n=== THEORY BOUND TRACKING ===\")\n",
    "theory_results = analysis_results['theory_bound_tracking']\n",
    "successful_theory = [r for r in theory_results.values() if r['status'] == 'success']\n",
    "if successful_theory:\n",
    "    ratios = [r['theory_ratio_final'] for r in successful_theory if r['theory_ratio_final'] is not None]\n",
    "    if ratios:\n",
    "        print(f\"Theory ratio - Mean: {np.mean(ratios):.3f}, Median: {np.median(ratios):.3f}\")\n",
    "        print(f\"Expected: O(1) when theory matches experiment\")\n",
    "        \n",
    "print(\"\\n=== STEPSIZE POLICY VALIDATION ===\")\n",
    "stepsize_results = analysis_results['stepsize_policy_validation']\n",
    "policy_pass = sum(1 for r in stepsize_results.values() if r['stepsize_policy_status'] == 'pass')\n",
    "policy_total = len([r for r in stepsize_results.values() if r['stepsize_policy_status'] in ['pass', 'fail']])\n",
    "if policy_total > 0:\n",
    "    print(f\"Stepsize policy adherence: {policy_pass}/{policy_total} runs passed\")\n",
    "    \n",
    "print(\"\\n=== PRIVACY & ODOMETER SANITY CHECKS ===\")\n",
    "privacy_results = analysis_results['privacy_odometer_checks']\n",
    "privacy_pass = sum(1 for r in privacy_results.values() if r['privacy_odometer_status'] == 'pass')\n",
    "privacy_total = len([r for r in privacy_results.values() if r['privacy_odometer_status'] in ['pass', 'fail']])\n",
    "if privacy_total > 0:\n",
    "    print(f\"Privacy/Odometer checks: {privacy_pass}/{privacy_total} runs passed\")\n",
    "    \n",
    "print(\"\\n=== SEED STABILITY AUDIT ===\")\n",
    "stability_results = analysis_results['seed_stability_audit']\n",
    "flagged_grids = [grid for grid, stats in stability_results.items() \n",
    "                if stats.get('high_variability_flag', False)]\n",
    "total_grids = len([s for s in stability_results.values() if s['status'] == 'success'])\n",
    "print(f\"High variability grids: {len(flagged_grids)}/{total_grids}\")\n",
    "if flagged_grids:\n",
    "    print(f\"Flagged grids: {flagged_grids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c12c24",
   "metadata": {},
   "source": [
    "## 6) One-page \u201cclaim check\u201d (export)\n",
    "Emits a compact JSON summary to artifacts/ for CI diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT = REPO/\"artifacts\"/(Path(\"04_stepsize_policy_ablation.ipynb\").stem + \"_claim_check.json\")\n",
    "ARTIFACT.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get base summary\n",
    "summary = con.execute(\"\"\"\n",
    "SELECT\n",
    "  dr.grid_id, dr.seed,\n",
    "  MAX(fe.cum_regret) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_cum_regret,\n",
    "  MAX(fe.cum_regret_with_noise) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_cum_regret_with_noise,\n",
    "  MAX(fe.P_T_true) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_P_T_true,\n",
    "  MAX(fe.ST_running) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_ST_running,\n",
    "  MAX(fe.rho_spent) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_rho_spent,\n",
    "  MAX(fe.m_used) FILTER (WHERE fe.event_id = (SELECT MAX(event_id) FROM analytics.fact_event WHERE grid_id = fe.grid_id AND seed = fe.seed)) AS final_m_used\n",
    "FROM analytics.fact_event fe\n",
    "JOIN analytics.dim_run dr USING (grid_id, seed)\n",
    "GROUP BY dr.grid_id, dr.seed\n",
    "ORDER BY dr.grid_id, dr.seed\n",
    "\"\"\").df().to_dict(orient=\"records\")\n",
    "\n",
    "# Enhance summary with standardized analysis results\n",
    "enhanced_summary = enhance_claim_check_export(\n",
    "    summary, \n",
    "    analysis_results[\"theory_bound_tracking\"],\n",
    "    analysis_results[\"stepsize_policy_validation\"],\n",
    "    analysis_results[\"privacy_odometer_checks\"],\n",
    "    analysis_results[\"seed_stability_audit\"]\n",
    ")\n",
    "\n",
    "ARTIFACT.write_text(json.dumps({\n",
    "    \"notebook\": \"04_stepsize_policy_ablation.ipynb\",\n",
    "    \"claim\": NOTEBOOK_CLAIM,\n",
    "    \"grid_file\": str(GRID_FILE),\n",
    "    \"summary\": enhanced_summary,\n",
    "    \"standardized_analyses\": {\n",
    "        \"theory_bound_tracking\": analysis_results[\"theory_bound_tracking\"],\n",
    "        \"stepsize_policy_validation\": analysis_results[\"stepsize_policy_validation\"],\n",
    "        \"privacy_odometer_checks\": analysis_results[\"privacy_odometer_checks\"],\n",
    "        \"seed_stability_audit\": analysis_results[\"seed_stability_audit\"]\n",
    "    }\n",
    "}, indent=2))\n",
    "print(\"Wrote:\", ARTIFACT)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}