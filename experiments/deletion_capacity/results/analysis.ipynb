{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a78bd9e",
   "metadata": {},
   "source": [
    "## Visualizing Results of the Deletion Capacity Experiment\n",
    "\n",
    "These are the results of the deletion capacity experiment. \n",
    "\n",
    "At a high level, we're seeing very conservative regret bounds for the Memory Pair. This means that we're requiring large sample complexity in return for a very low deletion capacity.\n",
    "\n",
    "It's also worth noting that our sample complexity (bar for a good learner) increases as the data wiggles more. When the Lipschitz constant and upper-bound on the Hessian are high, the sample complexity jumps and the amount of noise injected to the model becomes destabilizingly high.\n",
    "\n",
    "Goals:\n",
    "- Analyze the simulation results from the experiment runs and visualize the cumulative regret\n",
    "- Focus on $\\widehat{G}$ such that we can see its impact on the downstream stability of the learner\n",
    "- Investigate alternative methods of privacy accounting. Can we get tigheter regret bounds such that we don't inject so much noise into the parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb8ae3",
   "metadata": {},
   "source": [
    "\n",
    "### Page-Wide Questions\n",
    "- The formulas for sample complexity and deletion capacity look very similar (ie. use the $GD$ term). Why is this the case, and what does this suggest about the relationship between these two formulas? If I were to divide sample complexity by deletion capacity, it would almost look like something like a harmonic mean.\n",
    "- I wonder how $\\widehat{D}$ is being estimated. It looks like a lot of seeds are capping it at 10, which is a worst-case scenario. Is there something that can reduce this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e2ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588bb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/workspaces/unlearning-research-meta/experiments/deletion_capacity/results/grid_2025_08_11/sweep/gamma_1.0-split_0.5_q0.90_k5_relaxed_eps1.0/seed_000_synthetic_memorypair.csv\"\n",
    "\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b518deb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['C_hat', 'D_hat', 'G_hat', 'N_star_theory', 'P_T', 'P_T_est',\n",
       "       'S_scalar', 'acc', 'accountant_type', 'base_eta_t', 'c_hat',\n",
       "       'capacity_remaining', 'comparator_type', 'deletion_capacity',\n",
       "       'deletions_count', 'delta_step_theory', 'delta_total',\n",
       "       'drift_boost_remaining', 'drift_flag', 'eps_converted',\n",
       "       'eps_step_theory', 'eta_t', 'event', 'event_id', 'event_type',\n",
       "       'lambda_est', 'm_theory', 'op', 'regret', 'relaxation_factor',\n",
       "       'rho_remaining', 'rho_spent', 'rho_step', 'rho_total', 'sample_id',\n",
       "       'segment_id', 'sens_delete', 'sigma_step_base', 'sigma_step_theory',\n",
       "       'x_norm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a03fee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --sweep-dir SWEEP_DIR [--out-dir OUT_DIR]\n",
      "                             [--max-grids MAX_GRIDS] [--max-seeds MAX_SEEDS]\n",
      "                             [--write-report]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --sweep-dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/unlearning-research-meta/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analyze deletion-capacity runs (hashed grid IDs).\n",
    "- Reads sweep/manifest.(json|csv), per-grid params.json, seed_*.csv, seed_*_events.csv\n",
    "- Computes calibration/sample complexity, odometer/noise checks, regret vs bounds,\n",
    "  capacity alignment, drift responsiveness, and comparator/path-length checks.\n",
    "- Produces plots and CSVs under results/assessment/, and an optional Markdown summary.\n",
    "\n",
    "USAGE\n",
    "-----\n",
    "# typical (auto-detect manifest in <base-out>/sweep):\n",
    "python analyze_capacity.py --sweep-dir results/grid_2025_08_11/sweep --out-dir results/assessment\n",
    "\n",
    "# limit to a few grids/seeds for fast sanity:\n",
    "python analyze_capacity.py --sweep-dir results/grid_2025_08_11/sweep --max-grids 3 --max-seeds 2\n",
    "\n",
    "# also write a high-level Markdown summary:\n",
    "python analyze_capacity.py --sweep-dir results/grid_2025_08_11/sweep --write-report\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "results/assessment/\n",
    "  ├── summary_tables.csv                 # per (grid,seed) summary\n",
    "  ├── missing_fields.csv                 # schema presence matrix\n",
    "  ├── <grid_id>/\n",
    "  │    ├── seed_<seed>/\n",
    "  │    │    ├── regret_vs_bounds.png\n",
    "  │    │    ├── m_live_vs_emp.png\n",
    "  │    │    ├── noise_check_sample.csv\n",
    "  │    │    └── metrics.json\n",
    "  │    └── grid_summary.json\n",
    "  └── REPORT.md (if --write-report)\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from scipy.stats import spearmanr, kendalltau\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "\n",
    "# ----------------------------- IO helpers -----------------------------\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_manifest(sweep_dir: Path) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Return map: grid_id -> params dict.\"\"\"\n",
    "    jpath = sweep_dir / \"manifest.json\"\n",
    "    cpath = sweep_dir / \"manifest.csv\"\n",
    "    if jpath.exists():\n",
    "        return json.loads(jpath.read_text())\n",
    "    elif cpath.exists():\n",
    "        df = pd.read_csv(cpath)\n",
    "        out = {}\n",
    "        for _, row in df.iterrows():\n",
    "            gid = str(row[\"grid_id\"])\n",
    "            params = row.drop(labels=[\"grid_id\"]).to_dict()\n",
    "            out[gid] = params\n",
    "        return out\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"manifest.json/csv not found under {sweep_dir}\")\n",
    "\n",
    "def load_grid_params(grid_dir: Path) -> Dict[str, Any]:\n",
    "    p = grid_dir / \"params.json\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return json.loads(p.read_text())\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def find_seed_csvs(grid_dir: Path) -> List[Path]:\n",
    "    return sorted(grid_dir.glob(\"seed_*.csv\"))\n",
    "\n",
    "def find_event_csv(seed_csv: Path) -> Optional[Path]:\n",
    "    # expect sibling seed_<seed>_events.csv\n",
    "    events = seed_csv.with_name(seed_csv.stem + \"_events.csv\")\n",
    "    return events if events.exists() else None\n",
    "\n",
    "def coerce_numeric(v):\n",
    "    if isinstance(v, str):\n",
    "        try:\n",
    "            if any(c in v.lower() for c in (\".\", \"e\")):\n",
    "                return float(v)\n",
    "            return int(v)\n",
    "        except Exception:\n",
    "            return v\n",
    "    return v\n",
    "\n",
    "def first_present(d: Dict[str, Any], keys: List[str], default=None):\n",
    "    for k in keys:\n",
    "        if k in d and pd.notna(d[k]):\n",
    "            return d[k]\n",
    "    return default\n",
    "\n",
    "\n",
    "# ----------------------------- math helpers -----------------------------\n",
    "\n",
    "def red_ceil(x: float) -> int:\n",
    "    try:\n",
    "        return int(math.ceil(float(x)))\n",
    "    except Exception:\n",
    "        return int(x)\n",
    "\n",
    "def safe_spearman(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    if len(x) < 3 or len(y) < 3:\n",
    "        return np.nan\n",
    "    if HAVE_SCIPY:\n",
    "        r, _ = spearmanr(x, y, nan_policy=\"omit\")\n",
    "        return float(r)\n",
    "    # fallback: manual rank corr\n",
    "    xr = pd.Series(x).rank().to_numpy()\n",
    "    yr = pd.Series(y).rank().to_numpy()\n",
    "    xc = (xr - xr.mean()) / (xr.std() + 1e-12)\n",
    "    yc = (yr - yr.mean()) / (yr.std() + 1e-12)\n",
    "    return float(np.nanmean(xc * yc))\n",
    "\n",
    "def safe_kendall(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    if len(x) < 3 or len(y) < 3:\n",
    "        return np.nan\n",
    "    if HAVE_SCIPY:\n",
    "        r, _ = kendalltau(x, y, nan_policy=\"omit\")\n",
    "        return float(r)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# ----------------------------- analysis primitives -----------------------------\n",
    "\n",
    "def derive_gamma_split(params: Dict[str, Any], row: Dict[str, Any]) -> Tuple[Optional[float], Optional[float], Optional[float]]:\n",
    "    \"\"\"Return (gamma_bar, alpha, (gamma_ins, gamma_del)) using CSV columns first, else params.\"\"\"\n",
    "    gamma_bar = first_present(row, [\"gamma_bar\"], params.get(\"gamma_bar\"))\n",
    "    alpha = first_present(row, [\"gamma_split\", \"alpha\"], params.get(\"gamma_split\", params.get(\"alpha\")))\n",
    "    if gamma_bar is None or alpha is None:\n",
    "        return None, None, (None, None)\n",
    "    try:\n",
    "        gamma_bar = float(gamma_bar)\n",
    "        alpha = float(alpha)\n",
    "        gamma_ins = (1.0 - alpha) * gamma_bar\n",
    "        gamma_del = alpha * gamma_bar\n",
    "        return gamma_bar, alpha, (gamma_ins, gamma_del)\n",
    "    except Exception:\n",
    "        return None, None, (None, None)\n",
    "\n",
    "def rederive_N_star(G, D, c, C, gamma_ins) -> Optional[int]:\n",
    "    try:\n",
    "        val = (G * D * math.sqrt(c * C) / max(1e-12, gamma_ins)) ** 2\n",
    "        return red_ceil(val)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def adaptive_regret_bound(G, D, c, C, S_T) -> Optional[float]:\n",
    "    try:\n",
    "        return G * D * math.sqrt(c * C * S_T)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def static_regret_bound(G, lam, c, T) -> Optional[float]:\n",
    "    try:\n",
    "        return (G ** 2 / max(1e-12, lam * c)) * (1.0 + math.log(max(1, int(T))))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def dynamic_regret_bound(G, lam, c, T, P_T) -> Optional[float]:\n",
    "    stat = static_regret_bound(G, lam, c, T)\n",
    "    if stat is None or P_T is None:\n",
    "        return None\n",
    "    try:\n",
    "        return stat + G * P_T\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def recompute_sigma_epsdelta(sigma_base, eps_step, delta_step) -> Optional[float]:\n",
    "    \"\"\"sigma_base should be L / lambda_est; formula: base * sqrt(2 ln(1.25/delta_step))/eps_step\"\"\"\n",
    "    try:\n",
    "        return sigma_base * (math.sqrt(2.0 * math.log(1.25 / max(1e-18, float(delta_step)))) / max(1e-18, float(eps_step)))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def recompute_sigma_zcdp(sens_delete, rho_step) -> Optional[float]:\n",
    "    try:\n",
    "        return sens_delete / math.sqrt(2.0 * max(1e-18, float(rho_step)))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------- per-seed analysis -----------------------------\n",
    "\n",
    "def analyze_seed(\n",
    "    grid_id: str,\n",
    "    seed_csv: Path,\n",
    "    params: Dict[str, Any],\n",
    "    out_dir: Path\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns a metrics dict and writes per-seed artifacts to out_dir.\n",
    "    \"\"\"\n",
    "    _ensure_dir(out_dir)\n",
    "    df = pd.read_csv(seed_csv)\n",
    "\n",
    "    # Merge in params for missing columns\n",
    "    for k, v in params.items():\n",
    "        if k not in df.columns:\n",
    "            df[k] = v\n",
    "\n",
    "    # Best-effort dataset/accountant\n",
    "    dataset = first_present(df.iloc[-1].to_dict(), [\"dataset\"], params.get(\"dataset\", \"unknown\"))\n",
    "    acct = first_present(df.iloc[-1].to_dict(), [\"accountant\", \"accountant_type\"], params.get(\"accountant\", \"unknown\"))\n",
    "\n",
    "    # Basic signals\n",
    "    G = float(first_present(df.iloc[-1].to_dict(), [\"G_hat\"], np.nan) or np.nan)\n",
    "    D = float(first_present(df.iloc[-1].to_dict(), [\"D_hat\"], np.nan) or np.nan)\n",
    "    c = float(first_present(df.iloc[-1].to_dict(), [\"c_hat\"], np.nan) or np.nan)\n",
    "    C = float(first_present(df.iloc[-1].to_dict(), [\"C_hat\"], np.nan) or np.nan)\n",
    "    lam = float(first_present(df.iloc[-1].to_dict(), [\"lambda_est\", \"lambda_\"], params.get(\"lambda_\", np.nan)) or np.nan)\n",
    "    S_T = float(first_present(df.iloc[-1].to_dict(), [\"S_scalar\"], np.nan) or np.nan)\n",
    "\n",
    "    gamma_bar, alpha, (gamma_ins, gamma_del) = derive_gamma_split(params, df.iloc[-1].to_dict())\n",
    "\n",
    "    # Re-derive N*\n",
    "    N_star_logged = first_present(df.iloc[-1].to_dict(), [\"N_star_theory\", \"N_star_live\"], np.nan)\n",
    "    N_star_derived = rederive_N_star(G, D, c, C, gamma_ins) if all(np.isfinite([G, D, c, C, gamma_ins or np.nan])) else np.nan\n",
    "\n",
    "    # Capacity alignment (end-of-run)\n",
    "    m_theory = first_present(df.iloc[-1].to_dict(), [\"m_theory\", \"m_theory_live\"], np.nan)\n",
    "    m_emp = int((df[\"op\"] == \"delete\").sum()) if \"op\" in df.columns else np.nan\n",
    "\n",
    "    # Regret series (if per-event available, prefer that; otherwise cumsum if 'regret' is per-event already)\n",
    "    # Seed CSVs in your setup are per-event when using output_granularity=event; otherwise seed-level summaries can't plot timeseries.\n",
    "    # We handle both: if 'event_id' present and many rows, treat as event log.\n",
    "    is_event_like = \"event_id\" in df.columns and df[\"event_id\"].nunique() > 10\n",
    "    if is_event_like:\n",
    "        # cumulative regret\n",
    "        if \"regret\" in df.columns:\n",
    "            R_t = df[\"regret\"].cumsum().to_numpy()\n",
    "        else:\n",
    "            R_t = np.array([])\n",
    "        T = len(R_t)\n",
    "        # adaptive/static/dynamic bounds\n",
    "        R_adapt = adaptive_regret_bound(G, D, c, C, S_T) if np.isfinite(S_T) else np.nan\n",
    "        R_static = static_regret_bound(G, lam, c, T) if np.isfinite(lam) and np.isfinite(c) else np.nan\n",
    "        P_T = first_present(df.iloc[-1].to_dict(), [\"P_T\", \"P_T_est\"], np.nan)\n",
    "        R_dyn = dynamic_regret_bound(G, lam, c, T, P_T) if np.isfinite(P_T) else np.nan\n",
    "\n",
    "        # Plot regret vs bounds\n",
    "        if R_t.size > 0:\n",
    "            plt.figure()\n",
    "            plt.plot(np.arange(1, T + 1), R_t, label=\"R_T (empirical)\")\n",
    "            if np.isfinite(R_adapt): plt.axhline(R_adapt, linestyle=\"--\", label=\"R_adapt (bound)\")\n",
    "            if np.isfinite(R_static): plt.axhline(R_static, linestyle=\"--\", label=\"R_static (bound)\")\n",
    "            if np.isfinite(R_dyn): plt.axhline(R_dyn, linestyle=\"--\", label=\"R_dyn (bound)\")\n",
    "            plt.xlabel(\"Event\")\n",
    "            plt.ylabel(\"Cumulative regret\")\n",
    "            plt.title(f\"{grid_id} / seed {seed_csv.stem.split('_')[-1]} — regret vs bounds\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(out_dir / \"regret_vs_bounds.png\", dpi=150)\n",
    "            plt.close()\n",
    "    else:\n",
    "        R_t, R_adapt, R_static, R_dyn, T = np.array([]), np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Per-delete noise calibration sample\n",
    "    noise_sample = []\n",
    "    if \"op\" in df.columns:\n",
    "        deletes = df[df[\"op\"] == \"delete\"].copy()\n",
    "        if not deletes.empty:\n",
    "            # choose ~5 evenly spaced deletes\n",
    "            idxs = np.linspace(0, len(deletes) - 1, num=min(5, len(deletes)), dtype=int)\n",
    "            for _, row in deletes.iloc[idxs].iterrows():\n",
    "                acct_row = str(first_present(row.to_dict(), [\"accountant_type\", \"accountant\"], acct))\n",
    "                sigma_code = row.get(\"sigma_step_theory\", np.nan)\n",
    "                if acct_row in (\"eps_delta\", \"default\", \"legacy\"):\n",
    "                    sigma_base = row.get(\"sigma_step_base\", np.nan)  # expected L/lambda\n",
    "                    eps_step = row.get(\"eps_step_theory\", np.nan)\n",
    "                    delta_step = row.get(\"delta_step_theory\", np.nan)\n",
    "                    sigma_theory = recompute_sigma_epsdelta(sigma_base, eps_step, delta_step)\n",
    "                elif acct_row in (\"zcdp\", \"rdp\"):\n",
    "                    sens = row.get(\"sens_delete\", np.nan)\n",
    "                    rho_step = row.get(\"rho_step\", np.nan)\n",
    "                    sigma_theory = recompute_sigma_zcdp(sens, rho_step)\n",
    "                else:\n",
    "                    sigma_theory = np.nan\n",
    "                abs_err = np.nan\n",
    "                rel_err = np.nan\n",
    "                if np.isfinite(sigma_code) and sigma_theory is not None and np.isfinite(sigma_theory):\n",
    "                    abs_err = float(abs(sigma_code - sigma_theory))\n",
    "                    denom = max(1e-12, abs(sigma_theory))\n",
    "                    rel_err = float(abs_err / denom)\n",
    "                noise_sample.append({\n",
    "                    \"event_id\": int(row.get(\"event_id\", -1)) if \"event_id\" in row else -1,\n",
    "                    \"sigma_code\": sigma_code,\n",
    "                    \"sigma_recomputed\": sigma_theory,\n",
    "                    \"abs_err\": abs_err,\n",
    "                    \"rel_err\": rel_err,\n",
    "                    \"accountant_type\": acct_row,\n",
    "                    \"sens_delete\": row.get(\"sens_delete\", np.nan),\n",
    "                    \"eps_step_theory\": row.get(\"eps_step_theory\", np.nan),\n",
    "                    \"delta_step_theory\": row.get(\"delta_step_theory\", np.nan),\n",
    "                    \"rho_step\": row.get(\"rho_step\", np.nan),\n",
    "                })\n",
    "            # write sample\n",
    "            pd.DataFrame(noise_sample).to_csv(out_dir / \"noise_check_sample.csv\", index=False)\n",
    "\n",
    "    # Capacity vs deletes over time (if event log)\n",
    "    corr_m = np.nan\n",
    "    if is_event_like and \"m_theory\" in df.columns:\n",
    "        # build a time series of realized cumulative deletes\n",
    "        cum_del = (df[\"op\"] == \"delete\").astype(int).cumsum().to_numpy()\n",
    "        m_series = df[\"m_theory\"].to_numpy()\n",
    "        # spearman correlation across time\n",
    "        corr_m = safe_spearman(cum_del, m_series)\n",
    "        # plot\n",
    "        plt.figure()\n",
    "        plt.plot(cum_del, label=\"cumulative deletes (emp)\")\n",
    "        plt.plot(m_series, label=\"m_theory (live/logged)\")\n",
    "        if \"capacity_remaining\" in df.columns:\n",
    "            plt.plot(df[\"capacity_remaining\"].to_numpy(), label=\"capacity_remaining\")\n",
    "        plt.xlabel(\"Event\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"{grid_id} / seed {seed_csv.stem.split('_')[-1]} — capacity vs deletes\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir / \"m_live_vs_emp.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    # Schema presence (for quick matrix later)\n",
    "    required = [\"G_hat\", \"D_hat\", \"sigma_step_theory\"]\n",
    "    missing = [col for col in required if col not in df.columns]\n",
    "\n",
    "    # Summaries\n",
    "    metrics = {\n",
    "        \"grid_id\": grid_id,\n",
    "        \"seed\": int(seed_csv.stem.split(\"_\")[-1]),\n",
    "        \"dataset\": dataset,\n",
    "        \"accountant_type\": acct,\n",
    "        \"G_hat\": G, \"D_hat\": D, \"c_hat\": c, \"C_hat\": C, \"lambda_est\": lam,\n",
    "        \"S_scalar\": S_T,\n",
    "        \"gamma_bar\": gamma_bar, \"gamma_split\": alpha,\n",
    "        \"gamma_ins\": gamma_ins, \"gamma_del\": gamma_del,\n",
    "        \"N_star_logged\": float(N_star_logged) if pd.notna(N_star_logged) else np.nan,\n",
    "        \"N_star_derived\": float(N_star_derived) if N_star_derived is not None else np.nan,\n",
    "        \"N_star_rel_err\": (\n",
    "            abs(float(N_star_logged) - float(N_star_derived)) / max(1.0, float(N_star_derived))\n",
    "            if pd.notna(N_star_logged) and N_star_derived not in (None, 0, np.nan) else np.nan\n",
    "        ),\n",
    "        \"m_theory_final\": float(m_theory) if pd.notna(m_theory) else np.nan,\n",
    "        \"m_emp_final\": float(m_emp) if pd.notna(m_emp) else np.nan,\n",
    "        \"m_rel_err\": (\n",
    "            abs(float(m_theory) - float(m_emp)) / max(1.0, float(m_theory))\n",
    "            if pd.notna(m_theory) and pd.notna(m_emp) and float(m_theory) > 0 else np.nan\n",
    "        ),\n",
    "        \"regret_T\": float(df[\"regret\"].sum()) if \"regret\" in df.columns else np.nan,\n",
    "        \"R_adapt\": float(adaptive_regret_bound(G, D, c, C, S_T)) if all(np.isfinite([G, D, c, C, S_T])) else np.nan,\n",
    "        \"R_static\": float(static_regret_bound(G, lam, c, len(df))) if all(np.isfinite([G, lam, c])) else np.nan,\n",
    "        \"R_dynamic\": float(dynamic_regret_bound(G, lam, c, len(df), first_present(df.iloc[-1].to_dict(), [\"P_T\", \"P_T_est\"], np.nan))) if all(np.isfinite([G, lam, c])) else np.nan,\n",
    "        \"spearman_m_live_vs_emp\": corr_m,\n",
    "        \"missing_required_fields\": \",\".join(missing) if missing else \"\",\n",
    "    }\n",
    "\n",
    "    # dump metrics for the seed\n",
    "    with open(out_dir / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2, sort_keys=True)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ----------------------------- per-grid analysis -----------------------------\n",
    "\n",
    "def analyze_grid(grid_id: str, grid_dir: Path, params: Dict[str, Any], out_root: Path, max_seeds: Optional[int]) -> List[Dict[str, Any]]:\n",
    "    grid_out = out_root / grid_id\n",
    "    _ensure_dir(grid_out)\n",
    "    # persist params for convenience\n",
    "    with open(grid_out / \"grid_summary.json\", \"w\") as f:\n",
    "        json.dump(params, f, indent=2, sort_keys=True)\n",
    "\n",
    "    seed_csvs = find_seed_csvs(grid_dir)\n",
    "    if max_seeds:\n",
    "        seed_csvs = seed_csvs[:max_seeds]\n",
    "\n",
    "    all_metrics = []\n",
    "    for seed_csv in seed_csvs:\n",
    "        seed_out = grid_out / seed_csv.stem\n",
    "        m = analyze_seed(grid_id, seed_csv, params, seed_out)\n",
    "        all_metrics.append(m)\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "# ----------------------------- report writer -----------------------------\n",
    "\n",
    "def write_summary_tables(all_metrics: List[Dict[str, Any]], out_dir: Path):\n",
    "    if not all_metrics:\n",
    "        return\n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    df.to_csv(out_dir / \"summary_tables.csv\", index=False)\n",
    "\n",
    "    # presence matrix\n",
    "    presence_rows = []\n",
    "    for m in all_metrics:\n",
    "        presence_rows.append({\n",
    "            \"grid_id\": m[\"grid_id\"],\n",
    "            \"seed\": m[\"seed\"],\n",
    "            \"has_G_hat\": not pd.isna(m.get(\"G_hat\", np.nan)),\n",
    "            \"has_D_hat\": not pd.isna(m.get(\"D_hat\", np.nan)),\n",
    "            \"has_sigma_step_theory\": (m.get(\"missing_required_fields\", \"\") == \"\"),\n",
    "        })\n",
    "    pd.DataFrame(presence_rows).to_csv(out_dir / \"missing_fields.csv\", index=False)\n",
    "\n",
    "def write_report(markdown_path: Path, summary_csv: Path):\n",
    "    lines = []\n",
    "    lines.append(\"# Deletion Capacity — Early Analysis Report\\n\")\n",
    "    lines.append(\"This report summarizes calibration, odometer, and regret checks across grids/seeds.\\n\")\n",
    "    lines.append(f\"- Summary table: `{summary_csv}`\\n\")\n",
    "    lines.append(\"\\n## Key Checks\\n\")\n",
    "    lines.append(\"- Re-derived N* within 5% of logged value.\\n\")\n",
    "    lines.append(\"- Noise recomputation rel-error ≤ 5% for sampled deletes.\\n\")\n",
    "    lines.append(\"- Spearman(m_theory vs cumulative deletes) ≥ 0.6 where event logs exist.\\n\")\n",
    "    lines.append(\"\\n## Artifacts\\n\")\n",
    "    lines.append(\"- Per-grid, per-seed plots under `results/assessment/<grid_id>/seed_<seed>/`.\\n\")\n",
    "    markdown_path.write_text(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "# ----------------------------- main -----------------------------\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--sweep-dir\", required=True, help=\"Path to results/.../sweep directory\")\n",
    "    ap.add_argument(\"--out-dir\", default=\"results/assessment\", help=\"Where to write plots/tables\")\n",
    "    ap.add_argument(\"--max-grids\", type=int, default=None, help=\"Limit number of grids\")\n",
    "    ap.add_argument(\"--max-seeds\", type=int, default=None, help=\"Limit number of seeds per grid\")\n",
    "    ap.add_argument(\"--write-report\", action=\"store_true\", help=\"Write a brief Markdown summary\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    sweep_dir = Path(args.sweep_dir)\n",
    "    out_dir = Path(args.out_dir)\n",
    "    _ensure_dir(out_dir)\n",
    "\n",
    "    manifest = load_manifest(sweep_dir)\n",
    "    grid_ids = sorted(manifest.keys())\n",
    "    if args.max_grids:\n",
    "        grid_ids = grid_ids[:args.max_grids]\n",
    "\n",
    "    all_metrics: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i, gid in enumerate(grid_ids, 1):\n",
    "        print(f\"[{i}/{len(grid_ids)}] Analyzing grid {gid}\")\n",
    "        grid_dir = sweep_dir / gid\n",
    "        if not grid_dir.exists():\n",
    "            print(f\"  ! grid dir missing: {grid_dir}\")\n",
    "            continue\n",
    "        # load per-grid params (prefer params.json, fallback to manifest entry)\n",
    "        params = load_grid_params(grid_dir)\n",
    "        if not params:\n",
    "            params = manifest[gid]\n",
    "        # coerce possible numeric strings\n",
    "        params = {k: coerce_numeric(v) for k, v in params.items()}\n",
    "\n",
    "        metrics = analyze_grid(gid, grid_dir, params, out_dir, args.max_seeds)\n",
    "        all_metrics.extend(metrics)\n",
    "\n",
    "    write_summary_tables(all_metrics, out_dir)\n",
    "    if args.write_report:\n",
    "        write_report(out_dir / \"REPORT.md\", out_dir / \"summary_tables.csv\")\n",
    "\n",
    "    print(f\"\\nDone. Artifacts at: {out_dir.resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf73f8a",
   "metadata": {},
   "source": [
    "The code performs a grid search over the experiment parameters. For each seed,\n",
    "1. **(Calibration.)** a `Calibrator` object draws a small sample of the data stream to estimate stream-attributes like the Lipschitz constant $L$, the upper and lower bound of the Hessian eigenvalues $C, C$, and the resulting sample complexity required to meet predefined accuracy goals.\n",
    "2. **(Warmup.)** the model is trained on a stream of samples until it reaches sample complexity. This sets the model up for success when we test deletions.\n",
    "3. **(Workload.)** a stream of interleaved insertions and deletions is passed to the model. It's expected to service the requests in the order they're given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295985d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot describe a DataFrame without columns",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m summary_statistics = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m summary_statistics.columns\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/unlearning-research-meta/.venv/lib/python3.11/site-packages/pandas/core/generic.py:11995\u001b[39m, in \u001b[36mNDFrame.describe\u001b[39m\u001b[34m(self, percentiles, include, exclude)\u001b[39m\n\u001b[32m  11753\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m  11754\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdescribe\u001b[39m(\n\u001b[32m  11755\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11758\u001b[39m     exclude=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  11759\u001b[39m ) -> Self:\n\u001b[32m  11760\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m  11761\u001b[39m \u001b[33;03m    Generate descriptive statistics.\u001b[39;00m\n\u001b[32m  11762\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m  11993\u001b[39m \u001b[33;03m    max            NaN      3.0\u001b[39;00m\n\u001b[32m  11994\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m11995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdescribe_ndframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  11996\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  11997\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  11998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  11999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpercentiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpercentiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12000\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mdescribe\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/unlearning-research-meta/.venv/lib/python3.11/site-packages/pandas/core/methods/describe.py:91\u001b[39m, in \u001b[36mdescribe_ndframe\u001b[39m\u001b[34m(obj, include, exclude, percentiles)\u001b[39m\n\u001b[32m     87\u001b[39m     describer = SeriesDescriber(\n\u001b[32m     88\u001b[39m         obj=cast(\u001b[33m\"\u001b[39m\u001b[33mSeries\u001b[39m\u001b[33m\"\u001b[39m, obj),\n\u001b[32m     89\u001b[39m     )\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     describer = \u001b[43mDataFrameDescriber\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDataFrame\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m result = describer.describe(percentiles=percentiles)\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDFrameT, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/unlearning-research-meta/.venv/lib/python3.11/site-packages/pandas/core/methods/describe.py:162\u001b[39m, in \u001b[36mDataFrameDescriber.__init__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28mself\u001b[39m.exclude = exclude\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m obj.columns.size == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot describe a DataFrame without columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    164\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(obj)\n",
      "\u001b[31mValueError\u001b[39m: Cannot describe a DataFrame without columns"
     ]
    }
   ],
   "source": [
    "summary_statistics = data.describe()\n",
    "summary_statistics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6715a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calibrate' 'warmup' 'insert' 'delete']\n",
      "event_type\n",
      "warmup       169493\n",
      "calibrate      2500\n",
      "insert         1932\n",
      "delete         1927\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get the individual event types\n",
    "print(data[\"event_type\"].unique())\n",
    "print(data[\"event_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc11c680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_hat</th>\n",
       "      <th>D_hat</th>\n",
       "      <th>G_hat</th>\n",
       "      <th>N_star_theory</th>\n",
       "      <th>acc</th>\n",
       "      <th>c_hat</th>\n",
       "      <th>capacity_remaining</th>\n",
       "      <th>delta_step_theory</th>\n",
       "      <th>delta_total</th>\n",
       "      <th>eps_spent</th>\n",
       "      <th>...</th>\n",
       "      <th>sigma_step_theory</th>\n",
       "      <th>gamma_learning</th>\n",
       "      <th>gamma_privacy</th>\n",
       "      <th>quantile</th>\n",
       "      <th>deletion_ratio</th>\n",
       "      <th>accountant_type</th>\n",
       "      <th>privacy_budget</th>\n",
       "      <th>seed</th>\n",
       "      <th>data_stream_type</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.995444e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.302350e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.028844e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.397093e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.504764e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175847</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>1.305034e+27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.997147</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175848</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175849</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>9.006921e+25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175850</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175851</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>7.102584e+26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175852 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        C_hat     D_hat     G_hat  N_star_theory           acc  c_hat  \\\n",
       "0         NaN       NaN       NaN            NaN  1.995444e+00    NaN   \n",
       "1         NaN       NaN       NaN            NaN  2.302350e+00    NaN   \n",
       "2         NaN       NaN       NaN            NaN  5.028844e+00    NaN   \n",
       "3         NaN       NaN       NaN            NaN  1.397093e+01    NaN   \n",
       "4         NaN       NaN       NaN            NaN  8.504764e+00    NaN   \n",
       "...       ...       ...       ...            ...           ...    ...   \n",
       "175847    1.0  4.710798  4.605077         1883.0  1.305034e+27    1.0   \n",
       "175848    1.0  4.710798  4.605077         1883.0           NaN    1.0   \n",
       "175849    1.0  4.710798  4.605077         1883.0  9.006921e+25    1.0   \n",
       "175850    1.0  4.710798  4.605077         1883.0           NaN    1.0   \n",
       "175851    1.0  4.710798  4.605077         1883.0  7.102584e+26    1.0   \n",
       "\n",
       "        capacity_remaining  delta_step_theory  delta_total  eps_spent  ...  \\\n",
       "0                      inf                NaN          NaN   0.000000  ...   \n",
       "1                      inf                NaN          NaN   0.000000  ...   \n",
       "2                      inf                NaN          NaN   0.000000  ...   \n",
       "3                      inf                NaN          NaN   0.000000  ...   \n",
       "4                      inf                NaN          NaN   0.000000  ...   \n",
       "...                    ...                ...          ...        ...  ...   \n",
       "175847                 NaN       1.426534e-08      0.00001   0.997147  ...   \n",
       "175848                 NaN       1.426534e-08      0.00001   0.998573  ...   \n",
       "175849                 NaN       1.426534e-08      0.00001   0.998573  ...   \n",
       "175850                 NaN       1.426534e-08      0.00001   1.000000  ...   \n",
       "175851                 NaN       1.426534e-08      0.00001   1.000000  ...   \n",
       "\n",
       "        sigma_step_theory  gamma_learning gamma_privacy  quantile  \\\n",
       "0                     NaN             0.5           0.5      0.90   \n",
       "1                     NaN             0.5           0.5      0.90   \n",
       "2                     NaN             0.5           0.5      0.90   \n",
       "3                     NaN             0.5           0.5      0.90   \n",
       "4                     NaN             0.5           0.5      0.90   \n",
       "...                   ...             ...           ...       ...   \n",
       "175847      195235.982215             0.5           0.5      0.90   \n",
       "175848      195235.982215             0.5           0.5      0.90   \n",
       "175849      195235.982215             0.5           0.5      0.90   \n",
       "175850      195235.982215             0.5           0.5      0.90   \n",
       "175851      195235.982215             0.5           0.5      0.90   \n",
       "\n",
       "       deletion_ratio  accountant_type  privacy_budget seed data_stream_type  \\\n",
       "0                   1           legacy             1.0    5        synthetic   \n",
       "1                   1           legacy             1.0    5        synthetic   \n",
       "2                   1           legacy             1.0    5        synthetic   \n",
       "3                   1           legacy             1.0    5        synthetic   \n",
       "4                   1           legacy             1.0    5        synthetic   \n",
       "...               ...              ...             ...  ...              ...   \n",
       "175847              1           legacy             1.0    0        synthetic   \n",
       "175848              1           legacy             1.0    0        synthetic   \n",
       "175849              1           legacy             1.0    0        synthetic   \n",
       "175850              1           legacy             1.0    0        synthetic   \n",
       "175851              1           legacy             1.0    0        synthetic   \n",
       "\n",
       "         algorithm  \n",
       "0       memorypair  \n",
       "1       memorypair  \n",
       "2       memorypair  \n",
       "3       memorypair  \n",
       "4       memorypair  \n",
       "...            ...  \n",
       "175847  memorypair  \n",
       "175848  memorypair  \n",
       "175849  memorypair  \n",
       "175850  memorypair  \n",
       "175851  memorypair  \n",
       "\n",
       "[175852 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf63d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_level_data = data.loc[data[\"event_type\"].isnull()]\n",
    "event_level_data = data.loc[~data[\"event_type\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd49f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_hat</th>\n",
       "      <th>D_hat</th>\n",
       "      <th>G_hat</th>\n",
       "      <th>N_star_theory</th>\n",
       "      <th>acc</th>\n",
       "      <th>c_hat</th>\n",
       "      <th>capacity_remaining</th>\n",
       "      <th>delta_step_theory</th>\n",
       "      <th>delta_total</th>\n",
       "      <th>eps_spent</th>\n",
       "      <th>...</th>\n",
       "      <th>sigma_step_theory</th>\n",
       "      <th>gamma_learning</th>\n",
       "      <th>gamma_privacy</th>\n",
       "      <th>quantile</th>\n",
       "      <th>deletion_ratio</th>\n",
       "      <th>accountant_type</th>\n",
       "      <th>privacy_budget</th>\n",
       "      <th>seed</th>\n",
       "      <th>data_stream_type</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.995444e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.302350e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.028844e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.397093e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.504764e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175847</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>1.305034e+27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.997147</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175848</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175849</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>9.006921e+25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175850</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175851</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710798</td>\n",
       "      <td>4.605077</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>7.102584e+26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426534e-08</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>195235.982215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>legacy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>memorypair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175852 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        C_hat     D_hat     G_hat  N_star_theory           acc  c_hat  \\\n",
       "0         NaN       NaN       NaN            NaN  1.995444e+00    NaN   \n",
       "1         NaN       NaN       NaN            NaN  2.302350e+00    NaN   \n",
       "2         NaN       NaN       NaN            NaN  5.028844e+00    NaN   \n",
       "3         NaN       NaN       NaN            NaN  1.397093e+01    NaN   \n",
       "4         NaN       NaN       NaN            NaN  8.504764e+00    NaN   \n",
       "...       ...       ...       ...            ...           ...    ...   \n",
       "175847    1.0  4.710798  4.605077         1883.0  1.305034e+27    1.0   \n",
       "175848    1.0  4.710798  4.605077         1883.0           NaN    1.0   \n",
       "175849    1.0  4.710798  4.605077         1883.0  9.006921e+25    1.0   \n",
       "175850    1.0  4.710798  4.605077         1883.0           NaN    1.0   \n",
       "175851    1.0  4.710798  4.605077         1883.0  7.102584e+26    1.0   \n",
       "\n",
       "        capacity_remaining  delta_step_theory  delta_total  eps_spent  ...  \\\n",
       "0                      inf                NaN          NaN   0.000000  ...   \n",
       "1                      inf                NaN          NaN   0.000000  ...   \n",
       "2                      inf                NaN          NaN   0.000000  ...   \n",
       "3                      inf                NaN          NaN   0.000000  ...   \n",
       "4                      inf                NaN          NaN   0.000000  ...   \n",
       "...                    ...                ...          ...        ...  ...   \n",
       "175847                 NaN       1.426534e-08      0.00001   0.997147  ...   \n",
       "175848                 NaN       1.426534e-08      0.00001   0.998573  ...   \n",
       "175849                 NaN       1.426534e-08      0.00001   0.998573  ...   \n",
       "175850                 NaN       1.426534e-08      0.00001   1.000000  ...   \n",
       "175851                 NaN       1.426534e-08      0.00001   1.000000  ...   \n",
       "\n",
       "        sigma_step_theory  gamma_learning gamma_privacy  quantile  \\\n",
       "0                     NaN             0.5           0.5      0.90   \n",
       "1                     NaN             0.5           0.5      0.90   \n",
       "2                     NaN             0.5           0.5      0.90   \n",
       "3                     NaN             0.5           0.5      0.90   \n",
       "4                     NaN             0.5           0.5      0.90   \n",
       "...                   ...             ...           ...       ...   \n",
       "175847      195235.982215             0.5           0.5      0.90   \n",
       "175848      195235.982215             0.5           0.5      0.90   \n",
       "175849      195235.982215             0.5           0.5      0.90   \n",
       "175850      195235.982215             0.5           0.5      0.90   \n",
       "175851      195235.982215             0.5           0.5      0.90   \n",
       "\n",
       "       deletion_ratio  accountant_type  privacy_budget seed data_stream_type  \\\n",
       "0                   1           legacy             1.0    5        synthetic   \n",
       "1                   1           legacy             1.0    5        synthetic   \n",
       "2                   1           legacy             1.0    5        synthetic   \n",
       "3                   1           legacy             1.0    5        synthetic   \n",
       "4                   1           legacy             1.0    5        synthetic   \n",
       "...               ...              ...             ...  ...              ...   \n",
       "175847              1           legacy             1.0    0        synthetic   \n",
       "175848              1           legacy             1.0    0        synthetic   \n",
       "175849              1           legacy             1.0    0        synthetic   \n",
       "175850              1           legacy             1.0    0        synthetic   \n",
       "175851              1           legacy             1.0    0        synthetic   \n",
       "\n",
       "         algorithm  \n",
       "0       memorypair  \n",
       "1       memorypair  \n",
       "2       memorypair  \n",
       "3       memorypair  \n",
       "4       memorypair  \n",
       "...            ...  \n",
       "175847  memorypair  \n",
       "175848  memorypair  \n",
       "175849  memorypair  \n",
       "175850  memorypair  \n",
       "175851  memorypair  \n",
       "\n",
       "[175852 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_level_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70084b3",
   "metadata": {},
   "source": [
    "### What does this data mean?\n",
    "\n",
    "The data we get from the experiment is incredibly granular. This is good because we can isolate the impact of different operations on the regret. A list of the parameters is included below:\n",
    "\n",
    "`Data Stream Attributes`\n",
    "- $q$ is the quantile used for selecting the parameter estimates so we don't accidentally pull a high-ass parameter estimate\n",
    "- $\\widehat{C}$ is the upper bound on the Hessian eigenvalues\n",
    "- $\\widehat{C}$ is the lower bound on the Hessian eigenvalues\n",
    "- $\\widehat{D}$ is the upper bound of the diameter of the ellipsoid\n",
    "- $\\widehat{G}$ is the Lipschitz constant of the function, representing how much the output of the function changes as the inputs change\n",
    "- $N^{\\star}_{theory}$ is the theoretical sample complexity to reach the specified amount of average-regret\n",
    "\n",
    "`Workload Parameters`\n",
    "- $k$ is the number of insertions per delete operation\n",
    "- $m_{emp}$ is the empirical deletion capacity of the seed\n",
    "\n",
    "`Privacy Parameters`\n",
    "- $\\delta_{total}$ and $\\varepsilon_{total}$ are the total $(\\varepsilon,\\delta)$ budget given the the accountant\n",
    "- $\\delta_{step}$ and $\\varepsilon_{step}$ are the amount of privacy \"spent\" per deletion\n",
    "\n",
    "\n",
    "`Event-Level Attributes`\n",
    "- $event$ is the zero-based index of the operation within the seed run\n",
    "- $avg\\_regret\\_empirical$ is the mean per-operation regret for the stream of events up to this point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2ab42",
   "metadata": {},
   "source": [
    "## Theoretical Sample Complexities\n",
    "\n",
    "We can calculate theoretical sample complexities using the data we get from calibration. \n",
    "\n",
    "The formula for the sample complexity is based entirely on the attributes of our data stream and its spread: $G$, $D$, and $\\sqrt{cC}$ and so the estimates from our calibration period actually mean a lot. A large estimate for Lipschitz constant, or the bounds of our Hessian eigenvalues means we'll have an artificially inflated Sample Complexity.\n",
    "\n",
    "$$\n",
    "S = [\\frac{GD\\sqrt{Cc}}{\\gamma_{learn}}]^{2}\n",
    "$$\n",
    "\n",
    "It's also worth noting that the sample complexity is already quite conservative because of the method used for accouting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad19ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>N_star_theory</th>\n",
       "      <th>C_hat</th>\n",
       "      <th>c_hat</th>\n",
       "      <th>D_hat</th>\n",
       "      <th>G_hat</th>\n",
       "      <th>gamma_learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [seed, N_star_theory, C_hat, c_hat, D_hat, G_hat, gamma_learning]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_complexity_calculations = seed_level_data[[\"seed\", \"N_star_theory\", \"C_hat\", \"c_hat\", \"D_hat\", \"G_hat\", \"gamma_learning\"]]\n",
    "sample_complexity_calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c1e01",
   "metadata": {},
   "source": [
    "### Interpreting $\\gamma$ Parameters\n",
    "\n",
    "**Question:** What is the interpretation of $\\gamma_{learn}$ and how is it used to calculate sample complexity and deletion capacity?\n",
    "\n",
    "**Answer:** If $\\gamma_{learn}$ is the amount of slack given to the learner, then a $\\gamma_{learn}$ of `0.5` is really inflating my sample complexity by 4. Consider a larger $\\gamma_{learn}$ for the first round of experiments so that you don't blow up your sample complexity too early.\n",
    "\n",
    "The large sample complexities can also be an issue because our `max_events` parameter is set to 100000. So if the sample complexity is any larger than that, then the learner wouldn't even be able to unlearn a single point.\n",
    "\n",
    "**Question:** Okay, so we have two parameters $\\gamma_{learn}$ and $\\gamma_{private}$, why do we need them both? What's the difference between the learning parameter or the private parameter?\n",
    "\n",
    "**Answer:** They were separated because we need two separate slack parameters. One is used to bound the average regret during the learning period, and the second is used to bound the average regret when processing the workload.\n",
    "\n",
    "### Effects of Limited Convexity\n",
    "\n",
    "If the loss function is only weakly convex, then the experiment would end before the sample complexity is reached, and so even doing a single insertion would be a waste of time. I'm increasing the maximum number of events to allow for more of the experiments to reach this stage.\n",
    "\n",
    "**Note:** a suggestion would be to replace the two gamma parameters with a single $\\alpha$ that's used to split the amount of slack given to deletions versus insertions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c813ff",
   "metadata": {},
   "source": [
    "## Theoretical Deletion Capacities \n",
    "\n",
    "The $\\gamma_{priv}$ is also used to calculate deletion capacity. The quantifies the amount of cumulative regret you're willing to pay for all future deletions. It's used to calculate the upper bound on deletion capacity.\n",
    "\n",
    "$$\n",
    "m \\leq \\gamma_{priv} \\times \\frac{N^{\\star}}{GD + \\sigma\\sqrt{2N^{*}\\ln{\\frac{1}{\\delta_{step}}}}}\n",
    "$$\n",
    "\n",
    "The deletion capacity is only determined once the warmup has completed. We use the calibration statistics and the results from the warmup to calculate the theoretical deletion capacity for the experiment. This is the maximum number of deletions served (although many seeds never reach that point) and is used to calibrate the noise in the standard odometer.\n",
    "\n",
    "For some reason, we're not getting the $m_{theory}$ that we need to actually run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e21d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['m_emp', 'gamma_priv'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m deletion_capacity_data = \u001b[43mseed_level_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mm_theory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mm_emp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgamma_priv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mG_hat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD_hat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mN_star_theory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msigma_step_theory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      2\u001b[39m deletion_capacity_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/unlearning-research-meta/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/unlearning-research-meta/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/unlearning-research-meta/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['m_emp', 'gamma_priv'] not in index\""
     ]
    }
   ],
   "source": [
    "deletion_capacity_data = seed_level_data[[\"seed\", \"m_theory\",\"m_emp\", \"gamma_priv\", \"G_hat\", \"D_hat\", \"N_star_theory\", \"sigma_step_theory\"]]\n",
    "deletion_capacity_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65029edd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
