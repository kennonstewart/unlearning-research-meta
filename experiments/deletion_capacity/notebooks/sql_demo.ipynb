{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL Database Demo for Experiment Results\n",
    "\n",
    "This notebook demonstrates how to connect to the centralized PostgreSQL database and query experiment results.\n",
    "\n",
    "## Setup and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create database connection\n",
    "engine = sqlalchemy.create_engine(\"postgresql+psycopg2://unlearning:unlearning@localhost/unlearning_db\")\n",
    "\n",
    "print(\"Connected to PostgreSQL database!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how much data we have\n",
    "count_query = \"SELECT COUNT(*) as total_rows FROM fact_event;\"\n",
    "result = pd.read_sql(count_query, engine)\n",
    "print(f\"Total rows in database: {result['total_rows'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique experiment configurations\n",
    "config_query = \"\"\"\n",
    "SELECT DISTINCT \n",
    "    accountant, \n",
    "    gamma_bar, \n",
    "    gamma_split,\n",
    "    COUNT(*) as num_runs\n",
    "FROM fact_event \n",
    "WHERE accountant IS NOT NULL\n",
    "GROUP BY accountant, gamma_bar, gamma_split\n",
    "ORDER BY accountant, gamma_bar, gamma_split;\n",
    "\"\"\"\n",
    "\n",
    "configs = pd.read_sql(config_query, engine)\n",
    "print(\"Experiment configurations:\")\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Examples\n",
    "\n",
    "### Regret Analysis by Accountant Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regret across different accountant types\n",
    "regret_query = \"\"\"\n",
    "SELECT \n",
    "    accountant,\n",
    "    seed,\n",
    "    avg_regret_empirical,\n",
    "    N_star_emp,\n",
    "    m_emp,\n",
    "    final_acc\n",
    "FROM fact_event \n",
    "WHERE accountant IS NOT NULL \n",
    "    AND avg_regret_empirical IS NOT NULL\n",
    "ORDER BY accountant, seed;\n",
    "\"\"\"\n",
    "\n",
    "regret_data = pd.read_sql(regret_query, engine)\n",
    "print(f\"Loaded {len(regret_data)} regret observations\")\n",
    "print(regret_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regret by accountant type\n",
    "if len(regret_data) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for accountant in regret_data['accountant'].unique():\n",
    "        subset = regret_data[regret_data['accountant'] == accountant]\n",
    "        plt.scatter(subset['seed'], subset['avg_regret_empirical'], \n",
    "                   label=f'{accountant} (n={len(subset)})', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Seed')\n",
    "    plt.ylabel('Average Regret (Empirical)')\n",
    "    plt.title('Regret Performance by Accountant Type')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No regret data available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletion Capacity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze deletion capacity (m_emp) vs theoretical predictions\n",
    "deletion_query = \"\"\"\n",
    "SELECT \n",
    "    run_id,\n",
    "    seed,\n",
    "    accountant,\n",
    "    m_emp,\n",
    "    m_theory_live,\n",
    "    N_star_emp,\n",
    "    N_star_theory,\n",
    "    eps_spent,\n",
    "    eps_remaining\n",
    "FROM fact_event \n",
    "WHERE m_emp IS NOT NULL \n",
    "    AND accountant IS NOT NULL\n",
    "ORDER BY accountant, seed;\n",
    "\"\"\"\n",
    "\n",
    "deletion_data = pd.read_sql(deletion_query, engine)\n",
    "print(f\"Loaded {len(deletion_data)} deletion capacity observations\")\n",
    "print(deletion_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by accountant\n",
    "if len(deletion_data) > 0:\n",
    "    summary = deletion_data.groupby('accountant').agg({\n",
    "        'm_emp': ['mean', 'std', 'min', 'max'],\n",
    "        'N_star_emp': ['mean', 'std'],\n",
    "        'eps_spent': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"Summary statistics by accountant type:\")\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"No deletion data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Queries\n",
    "\n",
    "### Joining with Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lookup tables for cleaner queries\n",
    "lookup_query = \"\"\"\n",
    "SELECT \n",
    "    la.accountant_name,\n",
    "    COUNT(*) as num_experiments,\n",
    "    AVG(fe.avg_regret_empirical) as avg_regret,\n",
    "    AVG(fe.m_emp) as avg_deletions\n",
    "FROM fact_event fe\n",
    "JOIN lut_accountant la ON fe.accountant = la.accountant_name\n",
    "WHERE fe.avg_regret_empirical IS NOT NULL\n",
    "GROUP BY la.accountant_name\n",
    "ORDER BY avg_regret;\n",
    "\"\"\"\n",
    "\n",
    "lookup_results = pd.read_sql(lookup_query, engine)\n",
    "print(\"Aggregated results by accountant:\")\n",
    "print(lookup_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Schema Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available tables\n",
    "tables_query = \"\"\"\n",
    "SELECT table_name, table_type \n",
    "FROM information_schema.tables \n",
    "WHERE table_schema = 'public'\n",
    "ORDER BY table_name;\n",
    "\"\"\"\n",
    "\n",
    "tables = pd.read_sql(tables_query, engine)\n",
    "print(\"Available tables:\")\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show fact_event table structure\n",
    "columns_query = \"\"\"\n",
    "SELECT column_name, data_type, is_nullable \n",
    "FROM information_schema.columns \n",
    "WHERE table_name = 'fact_event' \n",
    "    AND table_schema = 'public'\n",
    "ORDER BY ordinal_position;\n",
    "\"\"\"\n",
    "\n",
    "columns = pd.read_sql(columns_query, engine)\n",
    "print(\"fact_event table structure:\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: SQL vs CSV Loading\n",
    "\n",
    "The traditional CSV loading approach requires:\n",
    "1. Manually finding and parsing multiple CSV files\n",
    "2. Concatenating data from different experiments  \n",
    "3. Handling inconsistent schemas across files\n",
    "4. Re-processing data for each analysis\n",
    "\n",
    "The PostgreSQL approach provides:\n",
    "1. **Centralized storage** - All experiment data in one place\n",
    "2. **Efficient queries** - SQL optimizations and indexing\n",
    "3. **Consistent schema** - Normalized tables with referential integrity\n",
    "4. **Concurrent access** - Multiple analysts can query simultaneously\n",
    "5. **Data validation** - Type checking and constraints\n",
    "6. **Incremental loading** - Add new experiments without reprocessing old data\n",
    "\n",
    "### Loading new data\n",
    "\n",
    "To load new CSV files into the database:\n",
    "\n",
    "```bash\n",
    "python etl_load.py --csv-dir experiments/deletion_capacity/results/new_grid \\\n",
    "    --dsn postgresql://unlearning:unlearning@localhost/unlearning_db\n",
    "```\n",
    "\n",
    "The ETL script handles:\n",
    "- Automatic discovery of CSV files in directory structure\n",
    "- Generation of unique run IDs\n",
    "- Batch insertion for performance\n",
    "- Conflict resolution (ON CONFLICT DO NOTHING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}