Produce a GitHub repo `memory-pair-accuracy` that runs the experiment:

  “Does test accuracy stay within 5 % of a fresh-retrain model
   while honouring up to m⋆ deletions?”

Environment
-----------
• Conda YAML `env.yml` (python=3.10, pytorch, scikit-learn,
  numpy, matplotlib, wandb)

Datasets
--------
1. CIFAR-10  (download via torchvision)
2. UCI COVTYPE

Experiment design
-----------------
* Warm-up: stream samples until sample-complexity threshold n_γ
  (γ = 0.05) detected by online running-loss estimator.
* Alternating cycle: 500 inserts → 50 deletes  (uniform random).
* Stop when Memory-Pair odometer ≤ 0 or baselines hit their static
  deletion bound.

Implement learners
------------------
• MemoryPairOnlineLBFGS  (ours)  
• SekhariBatch  (full retrain each delete)  
• QiaoHF        (HF delete, no new learning)  

Core script `run_accuracy.py`:
  --dataset {cifar10,covtype}
  --algo {memorypair,sekhari,qiao}
  --gamma 0.05
  --seed 7

Metrics logged via Weights & Biases:
  * test_top1_acc
  * odometer_remaining
  * cumulative_deletes
  * retrain_events

At the end export a CSV and a matplotlib figure
`figures/accuracy_vs_deletes_<dataset>.png`
with vertical dashed line at m⋆ (odometer zero).

README quick-start
------------------
```bash
git clone https://github.com/<USER>/memory-pair-accuracy
cd memory-pair-accuracy
conda env create -f env.yml
conda activate mp-acc
python run_accuracy.py --dataset cifar10 --algo memorypair --gamma 0.05

NOTE: here is some additional context for this task. This details the structure of the submodule in which you work and the structure of the data loader module from which the experiment will be ingesting data.

Create these files:

  README.md
  requirements.txt      (reuse root requirements; append seaborn)
  run.py
  schedules.py          (burst / trickle delete sched.)
  privacy.py            (simple ε,δ accountant & odometer)
  results/.gitkeep

Experiment spec
---------------
RQ: “How many deletions can each learner honour before retraining?”

Datasets: Rotating-MNIST only.
Algorithms:
  • MemoryPair (import from code.memory_pair…)
  • SekhariBatch  (dummy placeholder that retrains fully)
  • QiaoHF        (placeholder that pre-computes vectors then stops learning)

Delete-Insert cycle generated by schedules.py:
  --schedule burst   (500 inserts → 250 deletions repeat)
  --schedule trickle (1 % deletes every 100 inserts)

Metrics dumped into results/capacity_<algo>_<schedule>_<seed>.json:
  {steps, inserts, deletes, odometer_remaining, top1_acc}

run.py at end:
  git add results/*.json
  git commit -m "EXP:del_capacity <algo>-<schedule> <hash>"

README shows full repro and notes that MemoryPair must import
from code.memory_pair.src.memory_pair.

