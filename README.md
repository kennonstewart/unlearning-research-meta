# Research on Online Machine Unlearning

## Introduction

This repository contains the code and experiments for our research into the performance and characteristics of online machine unlearning algorithms. It serves as a central hub to ensure the full reproducibility of our findings.

This project investigates several key questions:

- Does the Memory-Pair learner achieve sub-linear cumulative regret on drifting data streams?
- What is the deletion capacity of privacy-preserving unlearning methods?
- How does model accuracy degrade as a function of sequential deletion operations?

## ğŸ“‚ Repository Structure

This repository is organized as a unified codebase with shared components and independent experiments:

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ AGENTS.md                    # Meta-repository guide for AI agents
â”œâ”€â”€ code/                        # Canonical, installable source code
â”‚   â”œâ”€â”€ memory_pair/            # Memory-Pair algorithm implementation
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ memory_pair.py  # Main algorithm with state machine
â”‚   â”‚       â”œâ”€â”€ calibrator.py   # Theoretical constants estimation
â”‚   â”‚       â”œâ”€â”€ odometer.py     # Privacy budget tracking
â”‚   â”‚       â”œâ”€â”€ lbfgs.py        # L-BFGS optimization
â”‚   â”‚       â””â”€â”€ metrics.py      # Performance metrics
â”‚   â”œâ”€â”€ data_loader/            # Unified dataset loaders
â”‚   â”‚   â”œâ”€â”€ mnist.py           # MNIST and Rotating-MNIST streams
â”‚   â”‚   â”œâ”€â”€ cifar10.py         # CIFAR-10 streams
â”‚   â”‚   â”œâ”€â”€ covtype.py         # Forest CoverType dataset
â”‚   â”‚   â”œâ”€â”€ linear.py          # Synthetic linear data
â”‚   â”‚   â”œâ”€â”€ streams.py         # Stream utilities
â”‚   â”‚   â””â”€â”€ utils.py           # Common utilities
â”‚   â””â”€â”€ baselines/             # Baseline algorithm implementations
â”œâ”€â”€ experiments/               # Independent experimental studies
â”‚   â”œâ”€â”€ deletion_capacity/     # Deletion capacity analysis
â”‚   â”œâ”€â”€ post_deletion_accuracy/ # Model accuracy degradation
â”‚   â””â”€â”€ sublinear_regret/      # Regret analysis on drifting streams
â””â”€â”€ paper/                     # Research paper materials
```

### Key Components

- **Memory-Pair Algorithm**: Implements a three-phase state machine (calibration, learning, interleaving) with privacy-preserving deletion capabilities
- **Data Loaders**: Unified, fail-safe dataset loaders for various machine learning benchmarks
- **Experiments**: Each subdirectory contains an independent study with its own `AGENTS.md` for specific instructions
- **Paper Materials**: LaTeX source and figures for the research paper in the `paper/` directory

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Virtual environment (recommended)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/kennonstewart/unlearning-research-meta.git
   cd unlearning-research-meta
   ```

2. **Set up the Python environment:**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install the packages in editable mode:**
   ```bash
   pip install -e code/memory_pair
   pip install -e code/data_loader
   pip install -e exp_engine
   ```

## ğŸ”¬ Key Features

### Memory-Pair Algorithm
- **Three-Phase State Machine**: Calibration â†’ Learning â†’ Interleaving
- **Automatic Calibration**: Estimates theoretical constants (G, D, c, C) from bootstrap data
- **Privacy-Preserving Deletions**: Uses differential privacy with regret-constrained optimization
- **Sample Complexity**: Automatically computes N* for optimal learning-to-prediction transition

### Privacy Odometer
- **Regret-Constrained Optimization**: Maximizes deletion capacity under regret bounds
- **Adaptive Noise Scaling**: Computes optimal Gaussian noise for each deletion
- **Budget Tracking**: Monitors Îµ and Î´ expenditure across deletions


## âœï¸ Development Guidelines

### Reproducibility
All scripts must be deterministic. Use the provided seeding utility:
```python
from code.data_loader.utils import set_global_seed
set_global_seed(args.seed)
```
